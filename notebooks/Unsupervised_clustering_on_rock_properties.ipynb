{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Unsupervised clustering on rock properties\n", "\n", "Sometimes we don't have labels, but would like to discover structure in a dataset. This is what clustering algorithms attempt to do. They don't require labels from us &mdash; they are 'unsupervised'.\n", "\n", "We'll use a subset of the [Rock Property Catalog](http://subsurfwiki.org/wiki/Rock_Property_Catalog) data, licensed CC-BY Agile Scientific. Note that the data have been preprocessed, including the addition of noise. See the notebook [RPC_for_regression_and_classification.ipynb](RPC_for_regression_and_classification.ipynb). \n", "\n", "We'll use two unsupervised techniques:\n", "\n", "- k-means clustering\n", "- DBSCAN\n", "\n", "We do have lithology labels for this dataset, so we can use those as a measure of how well we're doing with the clustering."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import pandas as pd\n", "\n", "%matplotlib inline\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df = pd.read_csv('../data/RPC_4_lithologies.csv')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df.describe()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Notice that the count of `Rho` values is smaller than for the other properties.\n", "\n", "Pairplots are a good way to see how the various features are distributed with respect to each other:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["sns.pairplot(df.dropna(), vars=['Vp', 'Vs', 'Rho_n'], hue='Lithology')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Clustering with _k_-means\n", "\n", "From [the Wikipedia article](https://en.wikipedia.org/wiki/K-means_clustering):\n", "\n", "> k-means clustering is a method of vector quantization, originally from signal processing, that is popular for cluster analysis in data mining. k-means clustering aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster. This results in a partitioning of the data space into Voronoi cells."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.cluster import KMeans"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["clu = KMeans()\n", "\n", "cols = ['Vp', 'Vs', 'Rho_n']\n", "\n", "clu.fit(df[cols].values)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The old classic: NaNs. Remember the count of `Rho` points being smaller than the others?\n", "\n", "The easiest thing to do, assuming we have the data, is to drop the rows with NaNs."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df = df.dropna()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["clu.fit(df[cols].values)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df['K means'] = clu.predict(df[cols].values)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for name, group in df.groupby('K means'):\n", "    plt.scatter(group.Vp, group.Rho_n, label=name)\n", "plt.legend()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We actually do have the labels, so let's compare..."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for name, group in df.groupby('Lithology'):\n", "    plt.scatter(group.Vp, group.Rho_n, label=name)\n", "plt.legend()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Clustering with DBSCAN\n", "\n", "DBSCAN has nothing to do with databases. From [the Wikipedia article](https://en.wikipedia.org/wiki/DBSCAN):\n", "\n", "> Density-based spatial clustering of applications with noise (DBSCAN) is [...] a density-based clustering algorithm: given a set of points in some space, it groups together points that are closely packed together (points with many nearby neighbors), marking as outliers points that lie alone in low-density regions (whose nearest neighbors are too far away). DBSCAN is one of the most common clustering algorithms and also most cited in scientific literature."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.cluster import DBSCAN\n", "\n", "DBSCAN()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["There are two important hyperparameters:\n", "\n", "- `eps`, the maximum distance between points in the same cluster.\n", "- `min_samples`, the minimum number of samples in a cluster."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["clu = DBSCAN(eps=150, min_samples=10)\n", "\n", "clu.fit(df[cols].values)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df['DBSCAN'] = clu.labels_"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for name, group in df.groupby('DBSCAN'):\n", "    plt.scatter(group.Vp, group.Rho_n, label=name)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["It's a bit hard to juggle the two parameters... let's make an interactive widget:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from ipywidgets import interact\n", "\n", "@interact(a=10, b=10)\n", "def add(a, b):\n", "    return a + b"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["@interact(eps=(10, 250, 10))\n", "def plot(eps):\n", "    clu = DBSCAN(eps=eps)\n", "    clu.fit(df[cols].values)\n", "    df['DBSCAN'] = clu.labels_\n", "    for name, group in df.groupby('DBSCAN'):\n", "        plt.scatter(group.Vp, group.Rho_n, label=name)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Manifold embedding with t-SNE\n", "\n", "[`t-SNE`](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) was recently implemented in `sklearn`. t-statistic neighbourhood embedding is a popular and very effective dimensionality reduction strategy. The caveat is that distance between clusters is not typically meaningful. But it's at least a useful data exploration tool.\n", "\n", "Usually we want `n_components=2`, so we get the data into a 2-space, e.g. for cross-plotting."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.manifold import TSNE\n", "\n", "tsne = TSNE(init='random', perplexity=15)\n", "\n", "embedding = tsne.fit_transform(df[cols].values)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.preprocessing import LabelEncoder\n", "\n", "labels = LabelEncoder().fit_transform(df.Lithology)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.scatter(*embedding.T, c=labels, cmap='tab10')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Another popular tool is UMAP. Check out [`umap-learn`](https://pypi.org/project/umap-learn/). You can install it with \n", "\n", "    conda install -y -c conda-forge umap-learn\n", "    \n", "It has the same interface as `sklearn` so it's very easy to use."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Taking it further"]}, {"cell_type": "markdown", "metadata": {}, "source": ["There are metrics for comparing clusterings. For example, `adjusted_rand_score` &mdash; from the scikit-learn docs:\n", "\n", "> The Rand Index computes a similarity measure between two clusterings by considering all pairs of samples and counting pairs that are assigned in the same or different clusters in the predicted and true clusterings.\n", ">\n", "> The raw RI score is then \u201cadjusted for chance\u201d into the ARI score using the following scheme:\n", "> \n", "> ARI = (RI - Expected_RI) / (max(RI) - Expected_RI)\n", "> \n", "> The adjusted Rand index is thus ensured to have a value close to 0.0 for random labeling independently of the number of clusters and samples and exactly 1.0 when the clusterings are identical (up to a permutation)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.metrics import adjusted_rand_score\n", "\n", "adjusted_rand_score(df.Lithology, df.DBSCAN)"]}, {"cell_type": "markdown", "metadata": {"tags": ["exercise"]}, "source": ["<div class=\"alert alert-success\">\n", "<h3>Exercises</h3>\n", "\n", "- Can you make the interactive widget display the Rand score? Use `plt.text(x, y, \"Text\")`.\n", "- Can you write a loop to find the value of `eps` giving the highest Rand score?\n", "- Can you add the `min_samples` parameter to the widget?\n", "- Explore some of [the other clustering algorithms](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.cluster).\n", "- Try some clustering on one of your own datasets (or use something from [sklearn](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.datasets), e.g. `sklearn.datasets.load_iris`).", "\n</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "geocomp-ml", "language": "python", "name": "geocomp-ml"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.7"}}, "nbformat": 4, "nbformat_minor": 2}