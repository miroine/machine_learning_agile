{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Intro to `tf.keras` for image classification\n", "\n", "We'll use the `keras` API included in `tensorflow` to create a model that predicts velocity curves from synthetic seismic images. The dataset comes from a collection of synthetic seismic samples put together by Lukas Mosser and released on Github as [SNIST](https://github.com/LukasMosser/SNIST). This dataset intends to be a standard benchmark similiar to MNIST. We'll look at a basic model first."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import matplotlib.pyplot as plt\n", "import numpy as np\n", "import datetime\n", "import os"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import warnings\n", "warnings.simplefilter(action='ignore', category=FutureWarning)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["urls = [\n", "        'https://raw.githubusercontent.com/LukasMosser/SNIST/master/data/train/train_amplitudes.npy',\n", "        'https://raw.githubusercontent.com/LukasMosser/SNIST/master/data/train/train_velocities.npy',\n", "        'https://raw.githubusercontent.com/LukasMosser/SNIST/master/data/test/test_amplitudes.npy',\n", "        'https://raw.githubusercontent.com/LukasMosser/SNIST/master/data/test/test_velocities.npy',\n", "        'https://raw.githubusercontent.com/LukasMosser/SNIST/master/data/test/test_amplitudes_noise_1.npy',\n", "        'https://raw.githubusercontent.com/LukasMosser/SNIST/master/data/test/test_amplitudes_noise_2.npy'\n", "    ]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Numpy allows you to point at URL data sources. It'll take care of downloading them and keeping reference of where they are with respect to a root folder specified by the user."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["ds = np.DataSource('../data/')\n", "\n", "train_amplitudes = np.load(ds.open(urls[0], mode='rb'))\n", "train_velocities = np.load(ds.open(urls[1], mode='rb'))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.imshow(train_amplitudes[0], aspect=0.06)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.plot(train_velocities[0])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["N_train = 600 #Number of total training examples\n", "N_val = 150 #Number of samples used for validation\n", "N_samples = 271 #Number of samples in time\n", "N_recorders = 20 #Number of recording stations\n", "N_layers = 9 #Number of layers in the target velocity model\n", "N_z = 360 #Number of grid blocks in z-dimension (only used for visualisation)\n", "\n", "lr = 1e-2 #Learning rate\n", "batch_size = N_train-N_val #Batchsize used in training - do full batch evaluation because of small data\n", "N_epochs = 200 #Number of epochs to train for"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We'll use Tensorflow's Keras functionality to setup a single layer network as baseline."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import tensorflow as tf\n", "from tensorflow.keras.layers import Dense\n", "from tensorflow.keras.optimizers import Adam, SGD"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["tf.version.VERSION"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's define a single layer network using `tf.keras.models` and `tf.keras.layers`."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model = tf.keras.models.Sequential()\n", "\n", "model.add(Dense(N_recorders*N_samples, input_dim=N_recorders*N_samples, activation='sigmoid'))\n", "model.add(Dense(50, activation='sigmoid'))\n", "model.add(Dense(N_layers))\n", "\n", "# Let's define an optimizer\n", "opt = SGD(lr=1e-2)\n", "\n", "model.compile(loss='mse',\n", "              optimizer=opt,\n", "              metrics=['mae'])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We'll need to standarize the input and normalize the output values"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["train_mean, train_std = train_amplitudes.mean(), train_amplitudes.std()\n", "train_vel_max = train_velocities.max()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X = train_amplitudes - train_mean\n", "X /= train_std\n", "\n", "y = train_velocities / train_vel_max\n", "\n", "X_train = X[:-N_val]\n", "y_train = y[:-N_val]\n", "\n", "\n", "X_test = X[N_train-N_val:]\n", "y_test = y[N_train-N_val:]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's confirm that the shapes of these matrices are OK:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_train.shape, y_train.shape, X_test.shape, y_test.shape"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We want to make the training data to be 1D to be allowed through this fully connected neural network."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_train = X_train.reshape((N_train-N_val, N_samples*N_recorders))\n", "X_test = X_test.reshape((N_val, N_samples*N_recorders))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In theory we could just do `model.fit(X_train, y_train)` and be done with it. But it's useful to setup a log system and callbacks to monitor the progress and quality of the training run."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["curr_date = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n", "log_dir = os.path.join(\n", "    \"logs\",\n", "    \"fit\",\n", "    curr_date,\n", ")\n", "\n", "model_name = 'seis_vel'\n", "\n", "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n", "es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n", "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(f'{log_dir}/{model_name}.h5', monitor='val_loss', verbose=1, save_best_only=True)\n", "\n", "\n", "model.fit(X_train, y_train,\n", "          epochs=10,\n", "          batch_size=N_train,\n", "          validation_data=(X_test, y_test),\n", "          callbacks=[tensorboard_callback,\n", "                     es_callback,\n", "                     checkpoint_callback,\n", "                    ],\n", "         )"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.plot(model.history.history['loss'])\n", "plt.plot(model.history.history['val_loss'])\n", "plt.gcf().set_size_inches(15,10)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["What if we want to load a pre-trained model?"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model.load_weights(f'logs/fit/{curr_date}/seis_vel.h5')\n", "y_pred = model.predict(X_test[0].reshape(1,*X_test[0].shape))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.plot(y_pred[0])\n", "plt.plot(y_test[0])"]}, {"cell_type": "markdown", "metadata": {"tags": ["exe"]}, "source": ["<div class=\"alert alert-success\">\n", "Create another model with more layers and more neurons per layer and train it using the same training data. Is it any better?", "\n</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# put your code here"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# How about a Convolutional Neural Network?"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model = tf.keras.Sequential()# Must define the input shape in the first layer of the neural network\n", "\n", "model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=2, padding='same', activation='relu', input_shape=(N_samples, N_recorders, 1))) \n", "model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n", "\n", "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\n", "model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n", "\n", "model.add(tf.keras.layers.Flatten())\n", "\n", "model.add(tf.keras.layers.Dense(256, activation='relu'))\n", "model.add(tf.keras.layers.Dropout(0.5))\n", "model.add(tf.keras.layers.Dense(N_layers))\n", "\n", "model.summary()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model.compile(loss='mse',\n", "              optimizer=opt,\n", "              metrics=['mae'])"]}, {"cell_type": "markdown", "metadata": {"tags": ["exe"]}, "source": ["<div class=\"alert alert-success\">\n", "To take full advantage of a convolutional neural network, the training data has to maintain its spatial relationships. In this case, the 2D images shouldn't be serialized. When this is the case, `keras` requires the training images to be in a very specific shape: (n_image, width, height, n_channels).\n", "\n", "**Exercise:**\n", "* Modify the shape of the X array to match the expected shape of the dataset. `X_train` should have the following shape afterwards: (450, 271, 20, 1).", "\n</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_train.shape"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["curr_date = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n", "log_dir = os.path.join(\n", "    \"logs_CNN\",\n", "    \"fit\",\n", "    curr_date,\n", ")\n", "\n", "model_name = 'seis_vel_CNN'\n", "\n", "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n", "es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n", "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(f'{log_dir}/{model_name}.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n", "\n", "\n", "model.fit(X_train, y_train,\n", "          epochs=10,\n", "          batch_size=N_train,\n", "          validation_data=(X_test, y_test),\n", "          callbacks=[tensorboard_callback,\n", "                     es_callback,\n", "                     checkpoint_callback,\n", "                    ],\n", "         )"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y_pred = model.predict(X_test[0].reshape(1,*X_test[0].shape))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.plot(y_pred[0])\n", "plt.plot(y_test[0])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# The fossil dataset\n", "Let's generate a workflow to classify images using a CNN.\n", "We'll make use of a collection of functions in `utils.py` to help process the images found in the `data/fossils` folder."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "from utils import make_train_test, preprocess_images"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X = np.load('../data/fossils/X.npy')\n", "y = np.load('../data/fossils/y.npy')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Or we can generate a training dataset from the image files themselves"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X, X_val, y, y_val = make_train_test(path='../data/fossils/prod/*/*/*.jpg', skip=['plants'])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X.shape"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We have 528 images of size 32x32. Let's look at one:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%matplotlib inline\n", "import matplotlib.pyplot as plt\n", "\n", "plt.imshow(X[2].reshape(32,32))\n", "plt.colorbar()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Shallow learning\n", "\n", "We can always use a shallow learning model on our dataset. This will give us something to beat with a neural network."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.ensemble import RandomForestClassifier\n", "from sklearn.metrics import f1_score\n", "\n", "clf = RandomForestClassifier(n_estimators=100)\n", "\n", "clf.fit(X, y)\n", "y_pred = clf.predict(X_val)\n", "f1_score(y_val, y_pred, average='weighted')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["About 65%... Pretty good!"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Neural network"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The network we're going to train wants a different shape of array. It needs a 4D array of shape `[instances, width, height, channels]`. So for us this will be `[528, 32, 32, 1]`. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_train = X.reshape(X.shape[0], int(np.sqrt(X.shape[1])), int(np.sqrt(X.shape[1])), 1) #channel last\n", "X_test = X_val.reshape(X_val.shape[0], int(np.sqrt(X_val.shape[1])), int(np.sqrt(X_val.shape[1])), 1)"]}, {"cell_type": "markdown", "metadata": {"tags": ["exe"]}, "source": ["<div class=\"alert alert-success\">\n", "Most neural network algorithms expect the target variables to be \"one hot encoded\". This means, a 1D vector for each label with a dimension of the number of classes. So, for our case with 4 distinct classes, `y[0] = array([1, 0, 0, 0])`, if `y[0]` belongs to the first class. \n", "\n", "**Exercise:**\n", "* Scikit-learn has encoders that can deal with this process automatically. Find the right encoder to produce the expected one hot encoded target vector.", "\n</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["We're ready to setup a network. We'll add layers to a `Sequential` keras network and we'll make use of a `scikit-learn` compatible `KerasClassifier` to build our training pipeline. First, the appropriate imports:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from tensorflow.keras.models import Sequential\n", "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n", "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n", "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n", "from sklearn.model_selection import GridSearchCV\n", "import joblib"]}, {"cell_type": "markdown", "metadata": {}, "source": ["To make life easy, we can define a function that buils the model. Inside this function there will be a collection of layers, activations and settings for each one."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["num_classes = 3\n", "\n", "img_rows, img_cols = 32, 32\n", "input_shape = (img_rows, img_cols, 1)\n", "\n", "\n", "def make_model(kernel_size=3, pool_size=2, filters=16, dense_layer_sizes=[16]):\n", "    model = Sequential()\n", "    \n", "    # Convolutional layers.\n", "    model.add(Conv2D(filters, kernel_size, padding='valid', input_shape=input_shape))\n", "    model.add(Activation('relu'))\n", "    model.add(Conv2D(filters, kernel_size))\n", "    model.add(Activation('relu'))\n", "    model.add(MaxPooling2D(pool_size=pool_size))\n", "    model.add(Dropout(0.25))\n", "\n", "    model.add(Flatten())\n", "    \n", "    # Dense layers.\n", "    for layer_size in dense_layer_sizes:\n", "        model.add(Dense(layer_size))\n", "        model.add(Activation('relu'))    \n", "    model.add(Dropout(0.5))\n", "\n", "    # Output layer.\n", "    model.add(Dense(num_classes))\n", "    model.add(Activation('softmax'))\n", "\n", "    model.compile(loss='categorical_crossentropy',\n", "                  optimizer='adadelta',\n", "                  metrics=['accuracy'])\n", "\n", "    return model"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Here's what the model will look like with all the default arguments; it has nearly 53,000 parameters."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model = make_model()\n", "\n", "model.summary()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Just as in the `scikit-learn` example, we can make use of `GridSearchCV` to setup a training run that can validate itself. This is just an example of that:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["dense_size_candidates = [[32], [16, 16]]  # Try a single Dense, and try two Dense layers\n", "\n", "kclf = KerasClassifier(make_model, batch_size=32)\n", "clf = GridSearchCV(kclf,\n", "                         param_grid={'dense_layer_sizes': dense_size_candidates,\n", "                                     # epochs is avail for tuning even when not\n", "                                     # an argument to model building function\n", "                                     'epochs': [3],\n", "                                     'filters': [8],\n", "                                     'kernel_size': [3],\n", "                                     'pool_size': [2]},\n", "                         scoring='neg_log_loss',\n", "                         n_jobs=1, cv=2)\n", "clf.fit(X_train, y_train)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can quickly look at some of the training metrics from the classification task:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# validator.best_estimator_ returns sklearn-wrapped version of best model.\n", "# validator.best_estimator_.model returns the (unwrapped) keras model\n", "best_model = clf.best_estimator_.model\n", "metric_names = best_model.metrics_names\n", "metric_values = best_model.evaluate(X_test, y_test) # included in keras\n", "for metric, value in zip(metric_names, metric_values):\n", "    print(metric, ': ', value)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["A couple of thins about predictiong from the `KerasClassifier` and the keras model directly:\n", "- The `scikit-learn` `keras` classifier doesn't return a one hot encoded prediction. It returns the index of class where the probability has a maximum value.\n", "- The `keras` model itself does return the `softmax` output: a 1D vector per image with the probability of each class membership."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["clf.predict(X_test)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["np.array([np.where(r==1)[0][0] for r in y_test])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Class probability\n", "\n", "The network can emit probabilities. Each instance's vector contains the probability of each class. The argmax of this gives the predicted class.\n", "\n", "In our poor result, the classes are almost equally likely."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y_probs = clf.predict_proba(X_val.reshape(-1, 32, 32, 1))\n", "y_probs[:10]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The `utils` module contains a utility for visualizing the results. The predicted class is shown with the prediction probability; underneath, the actual class is shown in brackets."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import utils\n", "\n", "utils.visualize(X_val, y_val, y_probs, ncols=5, nrows=3, shape=(32, 32))\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can still use the usual `scikit-learn` metrics to check the quality of the model:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.metrics import f1_score\n", "\n", "f1_score(np.array([np.where(r==1)[0][0] for r in y_test]), clf.predict(X_test), average='weighted')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The same idea of model persistance applies to `keras` models via `joblib`."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["clf.best_estimator_.model.save('fossil_model.h5')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Cloud power!\n", "\n", "Our network doesn't perform very well. Maybe more filters, more layers, or more training will help. Unfortunately, we'll quickly hit the ceiling of performance on our laptops. We need a bigger computer!\n", "\n", "Luckily, Google lets us use their computers, and even their GPUs and TPUs, for free in Google Colab.\n", "\n", "Here's a TensorFlow CNN running on Google Colab:\n", "\n", "### https://ageo.co/evRsvJ\n", "\n", "*Remember to change the Runtime to TPU.*"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Transfer Learning application"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We'll use `mobilenet` a popular image classification NN architecture to help classify our fossil images we'll use the weights trained by someone else!"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from tensorflow.keras.applications import MobileNet\n", "from tensorflow.keras.applications.mobilenet import preprocess_input\n", "from tensorflow.keras.models import Model\n", "from tensorflow.keras.preprocessing import image\n", "from tensorflow.keras.preprocessing.image import ImageDataGenerator"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We'll initialize a `MobileNet` model and we'll remove the last layer. In this way we can append our own last layer and only train that with our fossil images. Note that we're using a different way to \"add\" or connect layers. We're using the *functional* way of interacting with the Keras API:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["base_model = MobileNet(weights='imagenet', include_top=False) #imports the mobilenet model and discards the last 1000 neuron layer.\n", "\n", "x = base_model.output\n", "x = GlobalAveragePooling2D()(x)\n", "x = Dense(1024, activation='relu')(x) # We add dense layers so that the model can learn more complex functions and classify for better results.\n", "x = Dense(1024, activation='relu')(x) # Dense layer 2\n", "x = Dense(512, activation='relu')(x)  # Dense layer 3\n", "preds = Dense(4, activation='softmax')(x)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model = Model(inputs=base_model.input, outputs=preds)\n", "#specify the inputs\n", "#specify the outputs\n", "#now a model has been created based on our architecture"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can specify how many layers we want to freeze and how many we want the weights to be recalculated."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for layer in model.layers:\n", "    layer.trainable=False\n", "# or if we want to set the first 20 layers of the network to be non-trainable\n", "for layer in model.layers[:20]:\n", "    layer.trainable=False\n", "for layer in model.layers[20:]:\n", "    layer.trainable=True"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can also make use of `keras` included ability to process images for us. This will always save a significant amount of time."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input) #included in our dependencies\n", "\n", "train_generator = train_datagen.flow_from_directory('../data/fossils/mobilenet/train/',\n", "                                                    target_size=(224,224),\n", "                                                    color_mode='rgb',\n", "                                                    batch_size=32,\n", "                                                    class_mode='categorical',\n", "                                                    shuffle=True)\n", "\n", "val_generator = train_datagen.flow_from_directory('../data/fossils/mobilenet/val/',\n", "                                                  target_size=(224,224),\n", "                                                  color_mode='rgb',\n", "                                                  batch_size=77,\n", "                                                  class_mode='categorical',\n", "                                                  shuffle=False)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Finally, we just need to re-train the network given the setup specified."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n", "# Adam optimizer\n", "# loss function will be categorical cross entropy\n", "# evaluation metric will be accuracy\n", "\n", "step_size_train=train_generator.n//train_generator.batch_size\n", "model.fit_generator(generator=train_generator,\n", "                    steps_per_epoch=step_size_train,\n", "                    validation_data=val_generator,\n", "                    validation_steps=1,\n", "                    epochs=10)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's load an image and make a classification using a our model:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def load_image(img_path, show=False):\n", "\n", "    img = image.load_img(img_path, target_size=(224, 224))\n", "    img_tensor = image.img_to_array(img)                    # (height, width, channels)\n", "    img_tensor = np.expand_dims(img_tensor, axis=0)         # (1, height, width, channels), add a dimension because the model expects this shape: (batch_size, height, width, channels)\n", "    img_tensor /= 255.                                      # imshow expects values in the range [0, 1]\n", "\n", "    if show:\n", "        plt.imshow(img_tensor[0])                           \n", "        plt.axis('off')\n", "        plt.show()\n", "\n", "    return img_tensor"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["new_image = load_image('../data/fossils/mobilenet/val/ammonites/0076.jpg', True)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["np.argmax(model.predict(new_image))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["val_generator.class_indices"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# What if we just want to use a pre-trained model already trained:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from tensorflow.keras.preprocessing.image import load_img\n", "from tensorflow.keras.preprocessing.image import img_to_array\n", "from tensorflow.keras.applications.vgg19 import preprocess_input\n", "from tensorflow.keras.applications.vgg19 import decode_predictions\n", "from tensorflow.keras.applications.vgg19 import VGG19\n", "# load the model\n", "model = VGG19()\n", "# load an image from file\n", "image = load_img('../data/fossils/mobilenet/val/ammonites/0076.jpg', target_size=(224, 224))\n", "# convert the image pixels to a numpy array\n", "image = img_to_array(image)\n", "# reshape data for the model\n", "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n", "# prepare the image for the VGG model\n", "image = preprocess_input(image)\n", "# predict the probability across all output classes\n", "yhat = model.predict(image)\n", "# convert the probabilities to class labels\n", "label = decode_predictions(yhat)\n", "# retrieve the most likely result, e.g. highest probability\n", "label = label[0][0]\n", "# print the classification\n", "print(f'{label[1]} {label[2]*100:.2f}')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "deepl", "language": "python", "name": "deepl"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.7"}}, "nbformat": 4, "nbformat_minor": 4}